{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb439791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\mhtaba\\Desktop\\Ai_control2\\data_backup_47 (2).csv\"  # Update with actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Stripping spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "df = df.query('Et != 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0878d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "numbers = np.arange(1, 1633)\n",
    "\n",
    "# Select 10% of the elements randomly (without replacement)\n",
    "sample_size = int(0.1 * len(numbers))\n",
    "random_sample = np.random.choice(numbers, size=sample_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4d4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[~df['seed'].isin(random_sample)]\n",
    "df_filtered_test = df[df['seed'].isin(random_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d95edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1098, in __call__\n",
      "    self.retrieve()\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 975, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\multiprocessing\\pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\multiprocessing\\pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1320, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 757, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 67108864 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\mhtaba\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.97356055 0.97952156 0.98086672        nan 0.97642049 0.97726192\n",
      " 0.90574236 0.90629176 0.97958813 0.97968142]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2398\n",
      "[LightGBM] [Info] Number of data points in the train set: 565200, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 874.357236\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.982485</td>\n",
       "      <td>53.930802</td>\n",
       "      <td>47487.434861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.982302</td>\n",
       "      <td>60.05427</td>\n",
       "      <td>47984.620765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>{'num_leaves': 31, 'n_estimators': 200, 'max_d...</td>\n",
       "      <td>0.982045</td>\n",
       "      <td>95.843861</td>\n",
       "      <td>48682.097778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN</th>\n",
       "      <td>{'max_iter': 1000, 'learning_rate_init': 0.001...</td>\n",
       "      <td>0.988767</td>\n",
       "      <td>60.592273</td>\n",
       "      <td>30456.719436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Best Params  R2 Score  \\\n",
       "Extra Trees    {'n_estimators': 100, 'min_samples_split': 5, ...  0.982485   \n",
       "Random Forest  {'n_estimators': 200, 'min_samples_split': 2, ...  0.982302   \n",
       "LightGBM       {'num_leaves': 31, 'n_estimators': 200, 'max_d...  0.982045   \n",
       "ANN            {'max_iter': 1000, 'learning_rate_init': 0.001...  0.988767   \n",
       "\n",
       "                     MAE           MSE  \n",
       "Extra Trees    53.930802  47487.434861  \n",
       "Random Forest   60.05427  47984.620765  \n",
       "LightGBM       95.843861  48682.097778  \n",
       "ANN            60.592273  30456.719436  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Selecting features and target\n",
    "features = df_filtered.drop(columns=['Et', 'Ev', 'seed', 'city'])\n",
    "target = df_filtered['Et']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter grids\n",
    "et_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "lgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [20, 31, 50],\n",
    "    'max_depth': [-1, 10, 20]\n",
    "}\n",
    "ann_params = {\n",
    "    'hidden_layer_sizes': [(128, 64, 32), (256, 128, 64), (64, 32)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'max_iter': [200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Randomized Grid Search and Model Training\n",
    "def train_best_model(model, params, name):\n",
    "    search = RandomizedSearchCV(model, params, n_iter=10, cv=cv, scoring='r2', n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    return {\n",
    "        'Best Params': search.best_params_,\n",
    "        'R2 Score': r2_score(y_test, y_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results = {\n",
    "    'Extra Trees': train_best_model(ExtraTreesRegressor(random_state=42, n_jobs=-1), et_params, 'Extra Trees'),\n",
    "    'Random Forest': train_best_model(RandomForestRegressor(random_state=42, n_jobs=-1), rf_params, 'Random Forest'),\n",
    "    'LightGBM': train_best_model(lgb.LGBMRegressor(random_state=42), lgb_params, 'LightGBM'),\n",
    "    'ANN': train_best_model(MLPRegressor(activation='relu', solver='adam', random_state=42), ann_params, 'ANN')\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
